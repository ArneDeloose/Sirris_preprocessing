{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notebook for the pre-processing of the Sirris Wind Turbine data**\n",
    "\n",
    "-The data can be downloaded at https://opendata-renewables.engie.com/explore/index\n",
    "\n",
    "-Only the data files are needed (la-haute-borne-data-2013-2016.csv and la-haute-borne-data-2017-2020.csv)\n",
    "\n",
    "-Change the block below to the correct path of the folder that contains the downloaded data.\n",
    "\n",
    "-There are 7 steps in the pre-processing. Steps 3-6 are optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put the correct path to the data folder here\n",
    "data_path = 'C:/Users/adloose/Downloads/WindTurbines_Sirris'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 0: load packages and define functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#necessary packages\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#necessary functions\n",
    "def AD_imputeValues(data):\n",
    "    a=0\n",
    "    a_flag=True\n",
    "    min_val=0\n",
    "    max_val=0\n",
    "    for i in range(data.shape[0]):\n",
    "        check=np.sum(np.isnan(data[i, :]))\n",
    "        if check>0: #nan found\n",
    "            a=1 #at least 1 value is missing\n",
    "            a_flag=True\n",
    "            while a_flag:\n",
    "                if np.sum(np.isnan(data[i+a, :]))>0: #next point is also missing, continue checking\n",
    "                    a+=1 #one extra value missing\n",
    "                else: #next point exists, end while loop\n",
    "                    a_flag=False\n",
    "            for k in range(a+4): #for all missing values in a row\n",
    "                min_val=data[i-4, :] #just in case, go four values back\n",
    "                max_val=data[i+a, :] #i+a is the first valid value (right after the nan value)\n",
    "                #new value is:\n",
    "                data[i-4+k, :]=min_val+((k+1)*(max_val-min_val)/(a+5)) #start overwriting from i-4\n",
    "                #for 2 missing values, a=2 and k=0 and 1\n",
    "                #which means i+1 is min_val+(1/3)*(max_val-min_val)\n",
    "                #and i+2 is min_val+(2/3)*(max_val-min_val)\n",
    "    return(data)\n",
    "\n",
    "#interpolation function\n",
    "def AD_interpolate(num_points, data, min_index):\n",
    "    data_out=np.zeros((num_points, data.shape[1]),)\n",
    "    for i in range(num_points):\n",
    "        data_out[i, :]=data[min_index, :]+(i+1)*(data[min_index+1, :]-data[min_index, :])/(num_points+1)\n",
    "        #data is missing, so min_index is point before and max_index is the point right after that\n",
    "        #starting point: data[min_index, :]\n",
    "        #slope: (data[min_index+1, :]-data[min_index, :])/(num_points+1)\n",
    "        #location between start and stop: i+1\n",
    "        #for two points the result is: start point + 1/3(end_point-start_point) and start point + 2/3(end_point-start_point)\n",
    "    return(data_out)\n",
    "\n",
    "#double timestamp function\n",
    "def AD_doublestamp(data, s, t):\n",
    "    #s starting value, t: time\n",
    "    impute_data=np.zeros((t, data.shape[1]),)\n",
    "    for i in range(0, t*2, 2):\n",
    "        if math.isnan(data[s+i, 0]): #first value is missing\n",
    "            impute_data[i//2, :]=data[s+i+1, :] #put in second value\n",
    "        elif math.isnan(data[s+i+1, 0]): #second value is missing\n",
    "            impute_data[i//2, :]=data[s+i, :] #put in first value\n",
    "        else: #neither value is missing\n",
    "            impute_data[i//2, :]=data[s+i+1, :] #put in first value\n",
    "    data_out=np.concatenate((data[0:s, :], impute_data, data[(s+t*2):, :]), axis=0)\n",
    "    return(data_out)\n",
    "\n",
    "#triangle wave functions\n",
    "def s_func(x, offset):\n",
    "    x+=offset #make sure all angles start at 0 degrees\n",
    "    y=np.zeros((len(x),))\n",
    "    for i in range(len(x)):\n",
    "        if x[i]<=90: #0-90\n",
    "            y[i]=x[i]/90\n",
    "        elif x[i]>=270: #270-360\n",
    "            y[i]=(x[i]-360)/90\n",
    "        else: #90-270\n",
    "            y[i]=(180-x[i])/90    \n",
    "    return(y)\n",
    "\n",
    "def c_func(x, offset):\n",
    "    y=s_func(x, offset-90)\n",
    "    return(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: read in data and select colums**\n",
    "\n",
    "-The files are read, transformed to np arrays and joined together.\n",
    "\n",
    "-A second wind vane was installed in 2017. To keep the variables consistent, the data from these two windvanes is averaged into one variable.\n",
    "\n",
    "-Wind turbine and timestamp are saved into a seperate list. The remaining data is now numerical\n",
    "\n",
    "-A list of remaining variables is kept (original list: var_list1, new list: var_list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in all data\n",
    "data_1316 = pd.read_csv(data_path+'/la-haute-borne-data-2013-2016.csv', sep=';')\n",
    "var_list1=list(data_1316.columns.values) #list of all the variables\n",
    "data_1316 = data_1316.values #numpy array\n",
    "\n",
    "data_1720 = pd.read_csv(data_path+'/la-haute-borne-data-2017-2020.csv', sep=';')\n",
    "data_1720 = data_1720.values #numpy array\n",
    "\n",
    "#fill in Va colums in 1720 (second wind vane is installed in 2017)\n",
    "for i in range(data_1720.shape[0]):\n",
    "    if math.isnan(data_1720[i, 98]) and not math.isnan(data_1720[i, 90]): #if Va is missing and Va1 is present\n",
    "        for j in range(4):\n",
    "            data_1720[i, 98+j]=np.mean((data_1720[i, 90+j], data_1720[i, 94+j]), axis=0)\n",
    "\n",
    "#join 2013-2016 together with 2017-2020\n",
    "data_full=np.concatenate((data_1316, data_1720), axis=0)\n",
    "\n",
    "\n",
    "#turbine and timestamp\n",
    "turbine=data_full[:,0]\n",
    "timestamps=data_full[:,1]\n",
    "#extract useful part of timestamp\n",
    "for i in range(len(timestamps)):\n",
    "    timestamps[i]=timestamps[i][0:16]\n",
    "\n",
    "#convert to lists\n",
    "turbine=list(turbine)\n",
    "timestamps=list(timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select used data colums\n",
    "data=np.concatenate((data_full[:, 2:90], data_full[:, 98:126]), axis=1).astype('float64')\n",
    "#leave out 0 and 1 (time and turbine), 90-98 (windvane 1 and 2), and beyond 126 (empty)\n",
    "#make list variables\n",
    "var_list2=var_list1[2:90]+var_list1[98:126]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: chronological ordering**\n",
    "\n",
    "-The data is ordered chronologically and stored into arrays. The arrays are:\n",
    "\n",
    "data_WT1: turbine R80736\n",
    "\n",
    "WT2: R80721\n",
    "\n",
    "WT3: R80711\n",
    "\n",
    "WT4: R80790"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put everything in chronological order\n",
    "\n",
    "#pre-allocate a set of arrays. Keep in mind some timestamps could be missing, so we will make the arrays large enough.\n",
    "data_WT1=np.zeros((300000, 116), dtype='float64')\n",
    "data_WT2=np.zeros((300000, 116), dtype='float64')\n",
    "data_WT3=np.zeros((300000, 116), dtype='float64')\n",
    "data_WT4=np.zeros((300000, 116), dtype='float64')\n",
    "\n",
    "#we will count how many datapoints each wind turbine has.\n",
    "count1=0 #init counter\n",
    "count2=0 #init counter\n",
    "count3=0 #init counter\n",
    "count4=0 #init counter\n",
    "dummy_index=0\n",
    "\n",
    "timestamps_indexes=np.argsort(timestamps) #list of indexes in chronological order\n",
    "\n",
    "#go through every timestamp and allocate it\n",
    "for i in range(len(timestamps_indexes)):\n",
    "    dummy_index=timestamps_indexes[i] #correct index\n",
    "    #check which turbine it matches with (using the turbine list)\n",
    "    if turbine[dummy_index]=='R80736':\n",
    "        data_WT1[count1, :]=data[dummy_index, :]\n",
    "        count1+=1 #add 1 to counter\n",
    "    elif turbine[dummy_index]=='R80721':\n",
    "        data_WT2[count2, :]=data[dummy_index, :]\n",
    "        count2+=1\n",
    "    elif turbine[dummy_index]=='R80711':\n",
    "        data_WT3[count3, :]=data[dummy_index, :]\n",
    "        count3+=1\n",
    "    elif turbine[dummy_index]=='R80790':\n",
    "        data_WT4[count4, :]=data[dummy_index, :]\n",
    "        count4+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cut off extra zeros using counters\n",
    "data_WT1=data_WT1[0:count1, :]\n",
    "data_WT2=data_WT2[0:count2, :]\n",
    "data_WT3=data_WT3[0:count3, :]\n",
    "data_WT4=data_WT4[0:count4, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remaining steps will all overwrite these four data arrays, so any of them can be skipped or adapted. To write out the raw data from here, skip to step 7.\n",
    "\n",
    "**Step 3: imputing missing timestamps**\n",
    "\n",
    "-The missing timestamps were located manually (the chronological list can be checked). We found that 2 days and one extra timestamp are missing. These are interpolated lineary using the function that was defined earlier (interpolate function).\n",
    "\n",
    "-There is one set of double timestamps in WT1-WT3. These are dealt with using the function defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#impute missing timestamps\n",
    "\n",
    "#missing timestamps were located manually\n",
    "\n",
    "#custom function\n",
    "def AD_joining(data_WT):\n",
    "    data_20130127=AD_interpolate(1, data_WT, 3743)\n",
    "    data_20130304=AD_interpolate(144, data_WT, 8928)\n",
    "    #data_20130331=AD_interpolate(12, data_WT, 12826) #DST, not necessary\n",
    "    data_20130514=AD_interpolate(144, data_WT, 19152)\n",
    "    #data_20140330=AD_interpolate(12, data_WT, 64941)\n",
    "    #data_20150329=AD_interpolate(12, data_WT, 117344)\n",
    "    #data_20160327=AD_interpolate(12, data_WT, 169647)\n",
    "    #data_20170326=AD_interpolate(12, data_WT, 222049)\n",
    "    #data_20170610=AD_interpolate(144, data_WT, 232982) #double day, different function\n",
    "    data_WT_out=np.concatenate((data_WT[0:3744, :], data_20130127, data_WT[3744:8929, :], data_20130304,\n",
    "                            data_WT[8929:19153, :], data_20130514, data_WT[19153:, :]), axis=0)\n",
    "    return(data_WT_out)\n",
    "\n",
    "data_WT1=AD_joining(data_WT1)\n",
    "data_WT2=AD_joining(data_WT2)\n",
    "data_WT3=AD_joining(data_WT3)\n",
    "data_WT4=AD_joining(data_WT4)\n",
    "\n",
    "#fix the double timestamps mistake at 2017-06-10 in WT1-WT3\n",
    "data_WT1=AD_doublestamp(data_WT1, 233424, 144)\n",
    "data_WT2=AD_doublestamp(data_WT2, 233424, 144)\n",
    "data_WT3=AD_doublestamp(data_WT3, 233424, 144)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4: impute empty variables**\n",
    "\n",
    "-Empty variables are variables that have a nan value (which means the cell is present but is empty). This is not to be confused with the earlier missing timestamps, where a row is absent.\n",
    "\n",
    "-Based on experience, it is found that one missing variable will mean the other variables are less reliable too, so these are interpolated (and hence overwritten) as well. The overwrite also starts 4 positions before the nan-value because often these datapoints right before a nan are unreliable too. This can be changed in the AD_imputeValues function defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#impute empty variables\n",
    "data_WT1=AD_imputeValues(data_WT1)\n",
    "data_WT2=AD_imputeValues(data_WT2)\n",
    "data_WT3=AD_imputeValues(data_WT3)\n",
    "data_WT4=AD_imputeValues(data_WT4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5: angle conversions**\n",
    "\n",
    "-Because degrees are hard to deal with, all angles are converted with triangular functions. This is similar to a sine and a cosine function. This creates some new variables and removes some old ones. The new set of variables is stored in var_list3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert angles to triangle components\n",
    "\n",
    "#variables to be converted:\n",
    "list_c=list([0, 1, 2, 64, 65, 66, 84, 85, 86, 88, 89, 90])\n",
    "offset_list=list([120, 120, 120, 0, 0, 0, 0, 0, 0, 180, 180, 180]) #Ba has offset 120 and Va has offset 180\n",
    "var_angle_list=list()\n",
    "\n",
    "#delete variables (useless angle_std components):\n",
    "list_d=list([3, 67, 87, 91])\n",
    "\n",
    "triangle_components=np.zeros((data_WT1.shape[0], len(list_c)*2),) #init\n",
    "\n",
    "count=0\n",
    "for i in range(len(list_c)):\n",
    "    triangle_components[:, count]=s_func(data_WT1[:, list_c[i]], offset_list[i])\n",
    "    triangle_components[:, count+1]=c_func(data_WT1[:, list_c[i]], offset_list[i])\n",
    "    var_angle_list.append(var_list2[list_c[i]]+'_s')\n",
    "    var_angle_list.append(var_list2[list_c[i]]+'_c')\n",
    "    count+=2\n",
    "\n",
    "var_list3=var_list2.copy()\n",
    "list_del=list_c+list_d\n",
    "list_del.sort()\n",
    "for i in range(len(list_del)):\n",
    "    var_list3.pop(list_del[i]-i) #delete components one by one, keep in mind index starts to shift\n",
    "var_list3.extend(var_angle_list) #add new variables\n",
    "    \n",
    "#write out new variables in data array\n",
    "data_WT1=np.concatenate((data_WT1[:, 4:64], data_WT1[:, 68:84], data_WT1[:, 92:], triangle_components), axis=1)\n",
    "\n",
    "#repeat for WT2-4\n",
    "#2\n",
    "count=0\n",
    "for i in range(len(list_c)):\n",
    "    triangle_components[:, count]=s_func(data_WT2[:, list_c[i]], offset_list[i])\n",
    "    triangle_components[:, count+1]=c_func(data_WT2[:, list_c[i]], offset_list[i])\n",
    "    count+=2\n",
    "data_WT2=np.concatenate((data_WT2[:, 4:64], data_WT2[:, 68:84], data_WT2[:, 92:], triangle_components), axis=1)\n",
    "\n",
    "#3\n",
    "count=0\n",
    "for i in range(len(list_c)):\n",
    "    triangle_components[:, count]=s_func(data_WT3[:, list_c[i]], offset_list[i])\n",
    "    triangle_components[:, count+1]=c_func(data_WT3[:, list_c[i]], offset_list[i])\n",
    "    count+=2\n",
    "data_WT3=np.concatenate((data_WT3[:, 4:64], data_WT3[:, 68:84], data_WT3[:, 92:], triangle_components), axis=1)\n",
    "\n",
    "#4\n",
    "count=0\n",
    "for i in range(len(list_c)):\n",
    "    triangle_components[:, count]=s_func(data_WT4[:, list_c[i]], offset_list[i])\n",
    "    triangle_components[:, count+1]=c_func(data_WT4[:, list_c[i]], offset_list[i])\n",
    "    count+=2\n",
    "data_WT4=np.concatenate((data_WT4[:, 4:64], data_WT4[:, 68:84], data_WT4[:, 92:], triangle_components), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 6: normalisation**\n",
    "\n",
    "-All variables are normalised using the mean and the std. To make sure we do not divide by 0, we add 1e-5 to the std."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalise variables\n",
    "\n",
    "for i in range(data_WT1.shape[1]):\n",
    "    data_WT1[:, i]=(data_WT1[:, i]-np.mean(data_WT1[:, i]))/(np.std(data_WT1[:, i])+1e-5)\n",
    "\n",
    "for i in range(data_WT1.shape[1]):\n",
    "    data_WT2[:, i]=(data_WT2[:, i]-np.mean(data_WT2[:, i]))/(np.std(data_WT2[:, i])+1e-5)\n",
    "\n",
    "for i in range(data_WT1.shape[1]):\n",
    "    data_WT3[:, i]=(data_WT3[:, i]-np.mean(data_WT3[:, i]))/(np.std(data_WT3[:, i])+1e-5)\n",
    "\n",
    "for i in range(data_WT1.shape[1]):\n",
    "    data_WT4[:, i]=(data_WT4[:, i]-np.mean(data_WT4[:, i]))/(np.std(data_WT4[:, i])+1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 7: write out the data**\n",
    "\n",
    "-The data is written out to the original folder as WT1_full-WT4_full.csv\n",
    "\n",
    "-The variables are written out to a seperate csv file. Keep in mind this is a file without a header and var_list3 is used (the final set of variables). Earlier sets can be written out by changing this to var_list2 or var_list1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write out the data per wind turbine in a csv file\n",
    "np.savetxt(data_path+'/WT1_full.csv', data_WT1, delimiter=';')\n",
    "np.savetxt('C:/Users/adloose/Downloads/WindTurbines_Sirris/WT2_full.csv', data_WT2, delimiter=';')\n",
    "np.savetxt('C:/Users/adloose/Downloads/WindTurbines_Sirris/WT3_full.csv', data_WT3, delimiter=';')\n",
    "np.savetxt('C:/Users/adloose/Downloads/WindTurbines_Sirris/WT4_full.csv', data_WT4, delimiter=';')\n",
    "#list of variables\n",
    "df = pd.DataFrame(data={\"col1\": var_list3})\n",
    "df.to_csv('C:/Users/adloose/Downloads/WindTurbines_Sirris/var_list.csv', sep=';', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
